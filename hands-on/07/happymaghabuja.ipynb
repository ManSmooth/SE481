{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mansmooth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Bug prediction (pg 20-28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_worker(_stopwords, ps):\n",
    "    global stemmer\n",
    "    stemmer = ps\n",
    "    global stopword_set\n",
    "    stopword_set = _stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    cleaned_text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0'))\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    cleaned_text = ' '.join(['_variable_with_underscore' if '_' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_variable_with_dash' if '-' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_long_variable_name' if len(t) > 15 and t[0] != '#' else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_weburl' if t.startswith('http') and '/' in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number' if re.sub('[\\\\/;:_-]', '', t).isdigit() else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_variable_with_address' if re.match('.*0x[0-9a-f].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_name_with_number' if re.match('.*[a-f]*:[0-9]*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number_starts_with_one_character' if re.match('[a-f][0-9].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_number_starts_with_three_characters' if re.match('[a-f]{3}[0-9].*', t) else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_version' if any(i.isdigit() for i in t) and t.startswith('v') else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_localpath' if ('\\\\' in t or '/' in t) and ':' not in t else t for t in cleaned_text.split()])\n",
    "    cleaned_text = ' '.join(['_image_size' if t.endswith('px') else t for t in cleaned_text.split()])\n",
    "    tokenized_text = word_tokenize(cleaned_text)\n",
    "\n",
    "    sw_removed_text = [word for word in tokenized_text if word not in stopword_set]\n",
    "    sw_removed_text = [word for word in sw_removed_text if len(word) > 2]\n",
    "    stemmed_text = ' '.join([stemmer.stem(w) for w in sw_removed_text])\n",
    "\n",
    "    return stemmed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('resource/embold_train.json')\n",
    "dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "pool = Pool(8, initializer=initialize_worker, initargs=(stopwords_set, ps, ))\n",
    "\n",
    "cleaned_title = pool.map(preprocess, dataset.title)\n",
    "cleaned_body = pool.map(preprocess, dataset.body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts = pd.DataFrame([cleaned_title, cleaned_body], index=['title','body']).T\n",
    "y = dataset['label']\n",
    "\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = model_selection.train_test_split(data_texts.apply(lambda x: ' '.join([x[\"title\"], x[\"body\"]]), axis=1), y, test_size=0.1, stratify=y)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit)\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.976653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 372932\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5323\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.012230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 372733\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5306\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.972031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373110\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5326\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.989970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 372957\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5304\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.955620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373108\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5317\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "CV: p:0.8051 r:0.8025 f:0.8036\n"
     ]
    }
   ],
   "source": [
    "gbm_model = lgb.LGBMClassifier(boosting_type=\"dart\", num_leaves=63, verbosity=1)\n",
    "scoring = {\n",
    "    'precision': 'precision_macro',\n",
    "    'recall': 'recall_macro',\n",
    "    'f1': 'f1_macro'\n",
    "}\n",
    "scores = model_selection.cross_validate(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=1, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(scores[\"train_precision\"].mean(), scores[\"train_recall\"].mean(), scores[\"train_f1\"].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 42101, number of negative: 52399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.823405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 345174\n",
      "[LightGBM] [Info] Number of data points in the train set: 94500, number of used features: 4939\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445513 -> initscore=-0.218816\n",
      "[LightGBM] [Info] Start training from score -0.218816\n",
      "test: p:0.7807 r:0.7821 f:0.7814\n"
     ]
    }
   ],
   "source": [
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3, stratify=y_fit)\n",
    "\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train)\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test)\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest)\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "X_blindtest = gbm_model.predict(X_tfidf_blindtest)\n",
    "precision_test_score = metrics.precision_score(X_blindtest, y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(X_blindtest, y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(X_blindtest, y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vectorizer, open('resource/github_bug_prediction_tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(gbm_model, open('resource/github_bug_prediction_basic_model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_prob': 0.7695167191797154, 'predict_as': 'bug', 'status': 'success'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "res = requests.get(\"http://localhost:5000/predict_basic?title=download fail fail fail fail&body=download failed failed download\").text\n",
    "json.loads(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug_prob': 0.7770326760033179, 'predict_as': 'bug', 'status': 'success'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(\"http://localhost:5000/predict_basic?title=downloading failed&body=failed 404\").text\n",
    "json.loads(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Topic modelling (pg 39-44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=100, n_iter=10, random_state=0)\n",
    "X_lsa_fit = lsa.fit_transform(X_tfidf_fit)\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier(boosting_type=\"dart\", num_leaves=63, verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "fit: p:0.7787 r:0.7788 f:0.7787\n"
     ]
    }
   ],
   "source": [
    "scoring = {\n",
    "    'precision': 'precision_macro',\n",
    "    'recall': 'recall_macro',\n",
    "    'f1': 'f1_macro'\n",
    "}\n",
    "scores = model_selection.cross_validate(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=1, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(scores[\"train_precision\"].mean(), scores[\"train_recall\"].mean(), scores[\"train_f1\"].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.213167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397386\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5394\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.238721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398096\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5403\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.399082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398413\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5418\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.378145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397955\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5417\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.363662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398061\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5422\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "fit: p:0.8066 r:0.8054 f:0.8059\n"
     ]
    }
   ],
   "source": [
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "scores = model_selection.cross_validate(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=1, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(scores[\"train_precision\"].mean(), scores[\"train_recall\"].mean(), scores[\"train_f1\"].mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "X_tf_fit = count_vectorizer.transform(data_fit)\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest)\n",
    "lda = LatentDirichletAllocation(n_components=100, random_state=0)\n",
    "\n",
    "X_lda_fit = lda.fit_transform(X_tf_fit)\n",
    "gbm_model_with_lda = lgb.LGBMClassifier(boosting_type=\"dart\", num_leaves=63, verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "fit: p:0.7355 r:0.7341 f:0.7347\n"
     ]
    }
   ],
   "source": [
    "scoring = {\n",
    "    'precision': 'precision_macro',\n",
    "    'recall': 'recall_macro',\n",
    "    'f1': 'f1_macro'\n",
    "}\n",
    "scores = model_selection.cross_validate(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=1, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(scores[\"train_precision\"].mean(), scores[\"train_recall\"].mean(), scores[\"train_f1\"].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48116, number of negative: 59884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.175229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397386\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5394\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445519 -> initscore=-0.218795\n",
      "[LightGBM] [Info] Start training from score -0.218795\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.111096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398096\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5403\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.519618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 398413\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5418\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.301508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 397955\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5417\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "[LightGBM] [Info] Number of positive: 48115, number of negative: 59885\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.274820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 398061\n",
      "[LightGBM] [Info] Number of data points in the train set: 108000, number of used features: 5422\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445509 -> initscore=-0.218832\n",
      "[LightGBM] [Info] Start training from score -0.218832\n",
      "fit: p:0.8056 r:0.8039 f:0.8046\n"
     ]
    }
   ],
   "source": [
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr()\n",
    "scores = model_selection.cross_validate(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=1, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(scores[\"train_precision\"].mean(), scores[\"train_recall\"].mean(), scores[\"train_f1\"].mean()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se481-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
